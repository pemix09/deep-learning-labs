{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # algebra liniowa\n",
    "import matplotlib.pyplot as plt # używane do rysowania wykresów\n",
    "from sklearn.datasets import load_digits # importowanie zbioru danych\n",
    "from sklearn.model_selection import train_test_split # podział danych na część treningową i testową\n",
    "from sklearn.preprocessing import MinMaxScaler # normalizacja danych\n",
    "from sklearn.preprocessing import OneHotEncoder # kodowanie one-hot\n",
    "import optuna # do hiperparametryzacji\n",
    "import warnings # ignorowanie ostrzeżeń\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (64, 64)\n",
    "\n",
    "def load_train_data(input_dir, newSize=(64,64)):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from skimage.io import imread\n",
    "    import cv2 as cv\n",
    "    from pathlib import Path\n",
    "    import random\n",
    "    from shutil import copyfile, rmtree\n",
    "    import json\n",
    "\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import matplotlib\n",
    "    \n",
    "    image_dir = Path(input_dir)\n",
    "    categories_name = []\n",
    "    for file in os.listdir(image_dir):\n",
    "        d = os.path.join(image_dir, file)\n",
    "        if os.path.isdir(d):\n",
    "            categories_name.append(file)\n",
    "\n",
    "    folders = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n",
    "\n",
    "    train_img = []\n",
    "    categories_count=[]\n",
    "    labels=[]\n",
    "    for i, direc in enumerate(folders):\n",
    "        count = 0\n",
    "        for obj in direc.iterdir():\n",
    "            if os.path.isfile(obj) and os.path.basename(os.path.normpath(obj)) != 'desktop.ini':\n",
    "                labels.append(os.path.basename(os.path.normpath(direc)))\n",
    "                count += 1\n",
    "                img = imread(obj)#zwraca ndarry postaci xSize x ySize x colorDepth\n",
    "                img = cv.resize(img, newSize, interpolation=cv.INTER_AREA)# zwraca ndarray\n",
    "                img = img / 255#normalizacja\n",
    "                train_img.append(img)\n",
    "        categories_count.append(count)\n",
    "    X={}\n",
    "    X[\"values\"] = np.array(train_img)\n",
    "    X[\"categories_name\"] = categories_name\n",
    "    X[\"categories_count\"] = categories_count\n",
    "    X[\"labels\"]=labels\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "    # possible error cause - input size is not the same as image size \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, loss_func='mse'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        # initialize weights and biases:\n",
    "        self.input_hidden_layers_weights = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.hidden_layer_bias = np.zeros((1, self.hidden_size))\n",
    "        self.hidden_output_layers_weights = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.output_layer_bias = np.zeros((1, self.output_size))\n",
    "\n",
    "        # watch losses:\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'input layer size: {self.input_size}, hidden layer size: {self.hidden_size}, output size: {self.output_size}. Loss function: {self.loss_func}'\n",
    "    \n",
    "    # data is passed through the network - so hidden layer and output layer activations and weigted sums are calculated\n",
    "    def forward_propagation(self, x):\n",
    "        # x - input data (features) - pixels in our case\n",
    "        self.hidden_layer_weighted_sum = np.dot(x, self.input_hidden_layers_weights) + self.hidden_layer_bias\n",
    "        self.hidden_layer_activations = self.sigmoid(self.hidden_layer_weighted_sum)\n",
    "        self.output_layer_weighted_sum = np.dot(self.hidden_layer_activations, self.hidden_output_layers_weights) + self.output_layer_bias\n",
    "        if self.loss_func == 'categorical_crossentropy':\n",
    "            self.output_layer_activations = self.softmax(self.output_layer_weighted_sum)\n",
    "        else:\n",
    "            self.output_layer_activations = self.sigmoid(self.output_layer_weighted_sum)\n",
    "        return self.output_layer_activations\n",
    "    \n",
    "    # learning is done in this method\n",
    "    def backward_propagation(self, x, y, learning_rate):\n",
    "        m = x.shape[0]\n",
    "\n",
    "        # calculategradients\n",
    "        if self.loss_func == 'mse':\n",
    "            self.output_layer_gradient = self.output_layer_activations - y\n",
    "        elif self.loss_func == 'log_loss':\n",
    "            self.output_layer_gradient = -(y/self.output_layer_activations - (1-y)/(1-self.output_layer_activations))\n",
    "        elif self.loss_func == 'categorical_crossentropy':\n",
    "            self.output_layer_gradient = self.output_layer_activations - y\n",
    "        else:\n",
    "            raise ValueError('Not valid loss function! That ain\\'t work')\n",
    "\n",
    "        # calculate new weights              \n",
    "        self.new_output_layer_weights = (1/m) * np.dot(self.hidden_layer_activations.T, self.output_layer_gradient)\n",
    "        self.new_output_layer_bias =(1/m) * np.sum(self.output_layer_gradient, axis=0, keepdims=True)\n",
    "        self.hidden_layer_gradient = np.dot(self.output_layer_gradient, self.hidden_output_layers_weights.T) * self.sigmoid_derivative(self.hidden_layer_activations)\n",
    "        self.new_hidden_layer_weights = (1/m) * np.dot(x.T, self.hidden_layer_gradient)\n",
    "        self.new_hidden_layer_bias = (1/m) * np.sum(self.hidden_layer_gradient, axis=0, keepdims=True)\n",
    "\n",
    "        # update weights and biases, the algorithm don't take new weights as granted - so it's multiplied by learning_rate so it's 0.01(or less) * new_weights\n",
    "        self.hidden_output_layers_weights -= learning_rate * self.new_output_layer_weights\n",
    "        self.output_layer_bias -= learning_rate * self.new_output_layer_bias\n",
    "        self.input_hidden_layers_weights -= learning_rate * self.new_hidden_layer_weights\n",
    "        self.hidden_layer_bias -= learning_rate * self.new_hidden_layer_bias\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1-x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps/np.sum(exps, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loss_func='mse'):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        if self.loss_func == 'mse':\n",
    "            return np.mean((y_pred - y_true)**2)\n",
    "        elif self.loss_func == 'log_loss':\n",
    "            return -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
    "        elif self.loss_func == 'categorical_crossentropy':\n",
    "            return -np.mean(y_true*np.log(y_pred))\n",
    "        else:\n",
    "            raise ValueError('Nieprawidłowa funkcja straty')\n",
    "\n",
    "    def train(self, x_train, y_train, X_test, y_test, epochs, learning_rate):\n",
    "        for _ in range(epochs):\n",
    "            self.model.forward_propagation(x_train)\n",
    "            self.model.backward_propagation(x_train, y_train, learning_rate)\n",
    "            train_loss = self.calculate_loss(y_train, self.model.a2)\n",
    "            self.train_loss.append(train_loss)\n",
    "            \n",
    "            self.model.forward_propagation(X_test)\n",
    "            test_loss = self.calculate_loss(y_test, self.model.a2)\n",
    "            self.test_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "train_data = load_train_data('./train_test_sw/train_sw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance data function\n",
    "def enhance_train_data(data_to_augment, image_size=(64, 64)):\n",
    "   augmented_images = []\n",
    "   augmented_labels = []\n",
    "\n",
    "   for image,label in zip(data_to_augment['values'], data_to_augment['labels']):\n",
    "        augmented_images.append(image)\n",
    "        augmented_labels.append(label)\n",
    "        for _ in range(10):\n",
    "           augmented = augment(image, image_size)\n",
    "           augmented_images.append(augmented.numpy())\n",
    "           augmented_labels.append(label)\n",
    "   \n",
    "   return augmented_images, augmented_labels\n",
    "\n",
    "\n",
    "def augment(image, image_size=(64,64)):\n",
    "    import tensorflow as tf\n",
    "    delta = 0.09 # maximum relative change in brigtness\n",
    "    \n",
    "    image = tf.cast(image, tf.float64)\n",
    "    image = tf.image.random_crop(image, size=[image_size[0], image_size[1], 3])\n",
    "    image = tf.image.random_brightness(image, delta)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance data\n",
    "augmented_images, augmented_labels = enhance_train_data(train_data, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (11297, 64, 64) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[0;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_images\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mndarray)) \u001b[38;5;66;03m# <- possible error cause!!!\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(augmented_images))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(augmented_labels))\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (11297, 64, 64) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(np.array(np.array(augmented_images), dtype=np.ndarray)) # <- possible error cause!!!\n",
    "\n",
    "print(len(augmented_images))\n",
    "print(len(augmented_labels))\n",
    "# we don't extract any characteristis, only input is pixels\n",
    "# so i think the cause lays in the input size\n",
    "# either of pictures or something else\n",
    "\n",
    "# Prooces data and change it from list to array:\n",
    "y = np.array(augmented_labels)\n",
    "\n",
    "# Apply one hot encoding to categories names (output data)\n",
    "koder = OneHotEncoder()\n",
    "y_jednokodowane = koder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Podziel zbiór danych na zestawy treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_jednokodowane, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.12       0.        ]\n",
      " [0.04       0.04166667 0.12       0.        ]\n",
      " [0.16       0.16666667 0.24       0.        ]\n",
      " [0.16       0.25       0.36       0.        ]\n",
      " [0.12       0.08333333 0.12       0.        ]\n",
      " [0.12       0.04166667 0.         0.        ]\n",
      " [0.16       0.04166667 0.04       0.        ]\n",
      " [0.2        0.125      0.08       0.        ]\n",
      " [0.24       0.20833333 0.12       0.        ]\n",
      " [0.32       0.25       0.2        0.        ]\n",
      " [0.36       0.29166667 0.24       0.        ]\n",
      " [0.36       0.33333333 0.32       0.        ]\n",
      " [0.48       0.41666667 0.32       0.        ]\n",
      " [0.48       0.45833333 0.36       0.        ]\n",
      " [0.76       0.95833333 1.         0.        ]\n",
      " [0.56       0.45833333 0.4        0.        ]\n",
      " [0.84       0.75       0.52       0.        ]\n",
      " [1.         0.95833333 0.72       0.        ]\n",
      " [0.52       0.45833333 0.44       0.        ]\n",
      " [0.52       0.54166667 0.52       0.        ]\n",
      " [0.6        0.54166667 0.52       0.        ]\n",
      " [0.64       0.5        0.36       0.        ]\n",
      " [0.72       0.58333333 0.44       0.        ]\n",
      " [0.6        0.54166667 0.48       0.        ]\n",
      " [0.76       0.625      0.48       0.        ]\n",
      " [0.72       0.70833333 0.64       0.        ]\n",
      " [0.6        0.45833333 0.36       0.        ]\n",
      " [0.84       0.75       0.6        0.        ]\n",
      " [0.68       0.66666667 0.56       0.        ]\n",
      " [0.84       0.95833333 0.84       0.        ]\n",
      " [0.96       1.         0.92       0.        ]\n",
      " [0.88       0.875      0.8        0.        ]\n",
      " [0.88       0.83333333 0.76       0.        ]\n",
      " [0.84       0.70833333 0.64       0.        ]\n",
      " [0.84       0.91666667 0.92       0.        ]\n",
      " [0.8        0.83333333 0.8        0.        ]\n",
      " [0.76       0.70833333 0.64       0.        ]\n",
      " [0.8        0.75       0.68       0.        ]\n",
      " [0.92       0.91666667 0.88       0.        ]\n",
      " [0.8        0.91666667 0.96       0.        ]\n",
      " [0.68       0.75       0.76       0.        ]\n",
      " [0.72       0.66666667 0.68       0.        ]\n",
      " [0.68       0.66666667 0.72       0.        ]\n",
      " [0.56       0.54166667 0.52       0.        ]\n",
      " [0.52       0.58333333 0.6        0.        ]\n",
      " [0.44       0.5        0.56       0.        ]\n",
      " [0.52       0.45833333 0.48       0.        ]\n",
      " [0.52       0.45833333 0.4        0.        ]\n",
      " [0.48       0.45833333 0.36       0.        ]\n",
      " [0.52       0.54166667 0.52       0.        ]\n",
      " [0.36       0.29166667 0.24       0.        ]\n",
      " [0.4        0.41666667 0.4        0.        ]\n",
      " [0.32       0.29166667 0.24       0.        ]\n",
      " [0.64       0.625      0.56       0.        ]\n",
      " [0.28       0.25       0.2        0.        ]\n",
      " [0.36       0.29166667 0.16       0.        ]\n",
      " [0.28       0.25       0.2        0.        ]\n",
      " [0.24       0.20833333 0.2        0.        ]\n",
      " [0.4        0.375      0.4        0.        ]\n",
      " [0.28       0.25       0.28       0.        ]\n",
      " [0.16       0.125      0.12       0.        ]\n",
      " [0.12       0.04166667 0.04       0.        ]\n",
      " [0.08       0.04166667 0.04       0.        ]\n",
      " [0.08       0.04166667 0.08       0.        ]]\n",
      "64\n",
      "64\n",
      "input layer size: 64, hidden layer size: 64, output size: 5. Loss function: categorical_crossentropy\n"
     ]
    }
   ],
   "source": [
    "# Utwórz instancję klasy NeuralNetwork\n",
    "rozmiar_wejscia = X.shape[0] ### <- possible error cause!!! maybe we take the wrong shape!\n",
    "print(X)\n",
    "print(rozmiar_wejscia)\n",
    "print(X.shape[0])\n",
    "rozmiar_warstwy_ukrytej = 64\n",
    "rozmiar_wyjscia = len(np.unique(y))\n",
    "funkcja_straty = 'categorical_crossentropy'\n",
    "epoki = 1000\n",
    "wspolczynnik_uczenia = 0.1\n",
    "\n",
    "nn = MyNeuralNetwork(rozmiar_wejscia, rozmiar_warstwy_ukrytej, rozmiar_wyjscia, funkcja_straty)\n",
    "\n",
    "# Wyświetl architekturę sieci neuronowej\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (9037, 64, 64) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trener \u001b[38;5;241m=\u001b[39m Trainer(nn, funkcja_straty)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoki\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwspolczynnik_uczenia\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Przekonwertuj y_test z kodowania one-hot na etykiety\u001b[39;00m\n\u001b[1;32m      5\u001b[0m etykiety_y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, x_train, y_train, X_test, y_test, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_train, y_train, X_test, y_test, epochs, learning_rate):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 20\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbackward_propagation(x_train, y_train, learning_rate)\n\u001b[1;32m     22\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_loss(y_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39ma2)\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mMyNeuralNetwork.forward_propagation\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# x - input data (features) - pixels in our case\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_weighted_sum \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_hidden_layers_weights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_bias\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_weighted_sum)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_weighted_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_activations, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_output_layers_weights) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_bias\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (9037, 64, 64) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "trener = Trainer(nn, funkcja_straty)\n",
    "trener.train(X_train, y_train, X_test, y_test, epoki, wspolczynnik_uczenia)\n",
    "\n",
    "# Przekonwertuj y_test z kodowania one-hot na etykiety\n",
    "etykiety_y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Oceń wydajność sieci neuronowej\n",
    "prognozy = np.argmax(nn.forward_propagation(X_test), axis=1)\n",
    "dokladnosc = np.mean(prognozy == etykiety_y_test)\n",
    "print(f\"Dokładność: {dokladnosc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
