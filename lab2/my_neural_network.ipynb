{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # algebra liniowa\n",
    "import matplotlib.pyplot as plt # używane do rysowania wykresów\n",
    "from sklearn.datasets import load_digits # importowanie zbioru danych\n",
    "from sklearn.model_selection import train_test_split # podział danych na część treningową i testową\n",
    "from sklearn.preprocessing import MinMaxScaler # normalizacja danych\n",
    "from sklearn.preprocessing import OneHotEncoder # kodowanie one-hot\n",
    "import optuna # do hiperparametryzacji\n",
    "import warnings # ignorowanie ostrzeżeń\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (64, 64)\n",
    "\n",
    "def load_train_data(input_dir, newSize=(64,64)):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from skimage.io import imread\n",
    "    import cv2 as cv\n",
    "    from pathlib import Path\n",
    "    import random\n",
    "    from shutil import copyfile, rmtree\n",
    "    import json\n",
    "\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import matplotlib\n",
    "    \n",
    "    image_dir = Path(input_dir)\n",
    "    categories_name = []\n",
    "    for file in os.listdir(image_dir):\n",
    "        d = os.path.join(image_dir, file)\n",
    "        if os.path.isdir(d):\n",
    "            categories_name.append(file)\n",
    "\n",
    "    folders = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n",
    "\n",
    "    train_img = []\n",
    "    categories_count=[]\n",
    "    labels=[]\n",
    "    for i, direc in enumerate(folders):\n",
    "        count = 0\n",
    "        for obj in direc.iterdir():\n",
    "            if os.path.isfile(obj) and os.path.basename(os.path.normpath(obj)) != 'desktop.ini':\n",
    "                labels.append(os.path.basename(os.path.normpath(direc)))\n",
    "                count += 1\n",
    "                img = imread(obj)#zwraca ndarry postaci xSize x ySize x colorDepth\n",
    "                img = cv.resize(img, newSize, interpolation=cv.INTER_AREA)# zwraca ndarray\n",
    "                img = img / 255#normalizacja\n",
    "                train_img.append(img)\n",
    "        categories_count.append(count)\n",
    "    X={}\n",
    "    X[\"values\"] = np.array(train_img)\n",
    "    X[\"categories_name\"] = categories_name\n",
    "    X[\"categories_count\"] = categories_count\n",
    "    X[\"labels\"]=labels\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, loss_func='mse'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        # initialize weights and biases:\n",
    "        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias1 = np.zeros((1, self.hidden_size))\n",
    "        self.weights2 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias2 = np.zeros((1, self.output_size))\n",
    "\n",
    "        # watch losses:\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'input layer size: {self.input_size}, hidden layer size: {self.hidden_size}, output size: {self.output_size}. Loss function: {self.loss_func}'\n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        self.z1 = np.dot(x, self.weights1) + self.bias1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
    "        if self.loss_func == 'categorical_crossentrophy':\n",
    "            self.a2 = self.softmax(self.z2)\n",
    "        else:\n",
    "            self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward_propagation(self, x, y, learning_rate):\n",
    "        m = x.shape[0]\n",
    "\n",
    "        # gradients\n",
    "        if self.loss_func == 'mse':\n",
    "            self.dz2 = self.a2 - y\n",
    "        elif self.loss_func == 'log_loss':\n",
    "            self.dz2 = -(y/self.a2 - (1-y)/(1-self.a2))\n",
    "        elif self.loss_func == 'categorical_crossentropy':\n",
    "            self.dz2 = self.a2 - y\n",
    "        else:\n",
    "            raise ValueError('Not valid loss function! That ain\\'t work')\n",
    "\n",
    "        # calculate new weights                   \n",
    "        self.dw2 = (1/m) * np.dot(self.a1.T, self.dz2)\n",
    "        self.db2 =(1/m) * np.sum(self.dz2, axis=0, keepdims=True)\n",
    "        self.dz1 = np.dot(self.dz2, self.weight2.T) * self.sigmoid_derivative(self.a1)\n",
    "        self.dw1 = (1/m) * np.dot(x.T, self.dz1)\n",
    "        self.db1 = (1/m) * np.sum(self.dz1, axis=0, keepdims=True)\n",
    "\n",
    "        # update weights and biases\n",
    "        self.weights2 -= learning_rate * self.dw2\n",
    "        self.bias2 -= learning_rate * self.db2\n",
    "        self.weights1 -= learning_rate * self.dw1\n",
    "        self.bias1 -= learning_rate * self.db1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1-x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps/np.sum(exps, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loss_func='mse'):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        if self.loss_func == 'mse':\n",
    "            return np.mean((y_pred - y_true)**2)\n",
    "        elif self.loss_func == 'log_loss':\n",
    "            return -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
    "        elif self.loss_func == 'categorical_crossentropy':\n",
    "            return -np.mean(y_true*np.log(y_pred))\n",
    "        else:\n",
    "            raise ValueError('Nieprawidłowa funkcja straty')\n",
    "\n",
    "    def train(self, x_train, y_train, X_test, y_test, epochs, learning_rate):\n",
    "        for _ in range(epochs):\n",
    "            self.model.forward_propagation(x_train)\n",
    "            self.model.backward_propagation(x_train, y_train, learning_rate)\n",
    "            train_loss = self.calculate_loss(y_train, self.model.a2)\n",
    "            self.train_loss.append(train_loss)\n",
    "            \n",
    "            self.model.forward_propagation(X_test)\n",
    "            test_loss = self.calculate_loss(y_test, self.model.a2)\n",
    "            self.test_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "train_data = load_train_data('./train_test_sw/train_sw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance data function\n",
    "def enhance_train_data(data_to_augment, image_size=(64, 64)):\n",
    "   augmented_images = []\n",
    "   augmented_labels = []\n",
    "\n",
    "   for image,label in zip(data_to_augment['values'], data_to_augment['labels']):\n",
    "        augmented_images.append(image)\n",
    "        augmented_labels.append(label)\n",
    "        for _ in range(10):\n",
    "           augmented = augment(image, image_size)\n",
    "           augmented_images.append(augmented.numpy())\n",
    "           augmented_labels.append(label)\n",
    "   \n",
    "   return augmented_images, augmented_labels\n",
    "\n",
    "\n",
    "def augment(image, image_size=(64,64)):\n",
    "    import tensorflow as tf\n",
    "    delta = 0.09 # maximum relative change in brigtness\n",
    "    \n",
    "    image = tf.cast(image, tf.float64)\n",
    "    image = tf.image.random_crop(image, size=[image_size[0], image_size[1], 3])\n",
    "    image = tf.image.random_brightness(image, delta)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance data\n",
    "augmented_images, augmented_labels = enhance_train_data(train_data, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(augmented_images[0][0])\n",
    "\n",
    "# Prooces data and change it from list to array:\n",
    "y = np.array(augmented_labels)\n",
    "\n",
    "# Apply one hot encoding to categories names (output data)\n",
    "koder = OneHotEncoder()\n",
    "y_jednokodowane = koder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Podziel zbiór danych na zestawy treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_images, y_jednokodowane, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input layer size: 4, hidden layer size: 64, output size: 5. Loss function: categorical_crossentropy\n"
     ]
    }
   ],
   "source": [
    "# Utwórz instancję klasy NeuralNetwork\n",
    "rozmiar_wejscia = X.shape[1]\n",
    "rozmiar_warstwy_ukrytej = 64\n",
    "rozmiar_wyjscia = len(np.unique(y))\n",
    "funkcja_straty = 'categorical_crossentropy'\n",
    "epoki = 1000\n",
    "wspolczynnik_uczenia = 0.1\n",
    "\n",
    "nn = MyNeuralNetwork(rozmiar_wejscia, rozmiar_warstwy_ukrytej, rozmiar_wyjscia, funkcja_straty)\n",
    "\n",
    "# Wyświetl architekturę sieci neuronowej\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (9037, 64, 64) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trener \u001b[38;5;241m=\u001b[39m Trainer(nn, funkcja_straty)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoki\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwspolczynnik_uczenia\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Przekonwertuj y_test z kodowania one-hot na etykiety\u001b[39;00m\n\u001b[1;32m      5\u001b[0m etykiety_y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[122], line 20\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, x_train, y_train, X_test, y_test, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_train, y_train, X_test, y_test, epochs, learning_rate):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 20\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbackward_propagation(x_train, y_train, learning_rate)\n\u001b[1;32m     22\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_loss(y_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39ma2)\n",
      "Cell \u001b[0;32mIn[121], line 23\u001b[0m, in \u001b[0;36mMyNeuralNetwork.forward_propagation\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias1\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz1)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights2) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias2\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (9037, 64, 64) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "trener = Trainer(nn, funkcja_straty)\n",
    "trener.train(X_train, y_train, X_test, y_test, epoki, wspolczynnik_uczenia)\n",
    "\n",
    "# Przekonwertuj y_test z kodowania one-hot na etykiety\n",
    "etykiety_y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Oceń wydajność sieci neuronowej\n",
    "prognozy = np.argmax(nn.forward_propagation(X_test), axis=1)\n",
    "dokladnosc = np.mean(prognozy == etykiety_y_test)\n",
    "print(f\"Dokładność: {dokladnosc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
